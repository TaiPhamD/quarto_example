---
title: "My Awesome Paper"
bibliography: 
    - chen2016.bibtex
csl: ama.csl
jupyter: python3
---

# Summary

Training a large deep learning model could benefit from recomputation @chen2016training of intermediate activations. This is a technique that allows us to save memory by recomputing intermediate activations during the backward pass. In this post, we will implement this technique in PyTorch and see how it can be used to train a large model.

The bibliography is generated from the bibtex file. The csl file is used to format the bibliography. You can find more CSL files [here](https://github.com/citation-style-language/styles)

# Example code block

See [this](https://quarto.org/docs/reference/cells/cells-jupyter.html) for more details on how to hide code cell and its output. In this example we hide the code cell by using echo: false in the code block.
    
```{python}
# | echo: false
import numpy as np
import pandas as pd
# plot sine wave
import matplotlib.pyplot as plt
x = np.arange(0, 10, 0.1)
y = np.sin(x)
plt.plot(x, y)
plt.show()
```

# Example of latex equation
$$
y = \sum_{i=1}^N x_i^2
$$